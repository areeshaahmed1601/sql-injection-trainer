"""
Explainability Analysis for SQL Injection Detection Model
Uses SHAP to explain model predictions
"""

import numpy as np
import pandas as pd
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import joblib

logger = logging.getLogger(__name__)

class ModelExplainability:
    """
    Provides explainability for SQL injection detection model using SHAP.
    """
    
    def __init__(self, model, feature_extractor):
        self.model = model
        self.feature_extractor = feature_extractor
        self.results_dir = Path("research/results/explainability")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # Feature names (based on your ml_detector.py)
        self.feature_names = [
            'query_length', 'spaces', 'special_chars', 'keywords',
            'has_union', 'has_select', 'has_or', 'has_comment',
            'has_semicolon', 'has_equals', 'has_quotes', 'has_always_true',
            'pattern_tautology', 'pattern_union', 'pattern_comment',
            'pattern_stacked', 'pattern_time',
            'single_quotes', 'double_quotes', 'has_admin', 'has_password',
            'has_info_schema'
        ]
    
    def analyze_feature_importance(self, X_sample: np.ndarray) -> Dict:
        """Analyze feature importance using model's built-in method"""
        logger.info("ðŸ“Š Analyzing feature importance...")
        
        if hasattr(self.model, 'feature_importances_'):
            importances = self.model.feature_importances_
            
            # Create feature importance dictionary
            feature_importance = dict(zip(self.feature_names, importances))
            
            # Sort by importance
            sorted_importance = sorted(
                feature_importance.items(),
                key=lambda x: x[1],
                reverse=True
            )
            
            results = {
                'top_features': sorted_importance[:10],
                'all_features': dict(sorted_importance),
                'timestamp': datetime.now().isoformat()
            }
            
            # Save results
            self._save_feature_importance(results)
            
            return results
        else:
            logger.warning("âš ï¸ Model doesn't have feature_importances_ attribute")
            return {}
    
    def explain_prediction(self, query: str) -> Dict:
        """Explain individual prediction"""
        logger.info(f"ðŸ” Explaining prediction for query: {query[:50]}...")
        
        # Extract features
        features = self.feature_extractor.extract_features(query)
        
        # Get prediction
        prediction = self.model.predict([features])[0]
        probability = self.model.predict_proba([features])[0]
        
        # Create explanation
        explanation = {
            'query': query[:100] + "..." if len(query) > 100 else query,
            'prediction': 'malicious' if prediction == 1 else 'benign',
            'confidence': float(max(probability)),
            'features': dict(zip(self.feature_names, [float(x) for x in features])),
            'top_contributors': self._identify_top_contributors(features),
            'timestamp': datetime.now().isoformat()
        }
        
        return explanation
    
    def _identify_top_contributors(self, features: np.ndarray) -> List[Dict]:
        """Identify top contributing features for a prediction"""
        
        if hasattr(self.model, 'feature_importances_'):
            importances = self.model.feature_importances_
            
            # Calculate contribution (feature value * importance)
            contributions = []
            for i, (name, importance) in enumerate(zip(self.feature_names, importances)):
                contribution = float(features[i] * importance)
                if contribution > 0.01:  # Only include significant contributions
                    contributions.append({
                        'feature': name,
                        'value': float(features[i]),
                        'importance': float(importance),
                        'contribution': contribution
                    })
            
            # Sort by contribution
            contributions.sort(key=lambda x: abs(x['contribution']), reverse=True)
            
            return contributions[:5]  # Top 5 contributors
        
        return []
    
    def generate_global_explanations(self, X_sample: np.ndarray, y_sample: np.ndarray):
        """Generate global explanations using SHAP (if available)"""
        logger.info("ðŸŒ Generating global explanations...")
        
        try:
            import shap
            
            # Create SHAP explainer
            explainer = shap.TreeExplainer(self.model)
            
            # Calculate SHAP values
            shap_values = explainer.shap_values(X_sample)
            
            # Save SHAP summary plot
            plt = self._create_shap_summary_plot(shap_values, X_sample)
            
            # Calculate feature importance from SHAP
            shap_importance = np.abs(shap_values).mean(0)
            shap_feature_importance = dict(zip(self.feature_names, shap_importance))
            
            results = {
                'shap_feature_importance': dict(sorted(
                    shap_feature_importance.items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:10]),
                'expected_value': float(explainer.expected_value[0]),
                'shap_values_sample': shap_values[0].tolist()[:5],  # First 5 samples
                'timestamp': datetime.now().isoformat()
            }
            
            self._save_shap_results(results)
            
            logger.info("âœ… Global explanations generated with SHAP")
            return results
            
        except ImportError:
            logger.warning("âš ï¸ SHAP not installed, using simpler analysis")
            return self.analyze_feature_importance(X_sample)
        except Exception as e:
            logger.error(f"âŒ Error in SHAP analysis: {e}")
            return {}
    
    def _create_shap_summary_plot(self, shap_values, X_sample):
        """Create and save SHAP summary plot"""
        try:
            import shap
            import matplotlib.pyplot as plt
            
            plt.figure(figsize=(12, 8))
            shap.summary_plot(
                shap_values,
                X_sample,
                feature_names=self.feature_names[:X_sample.shape[1]],
                show=False,
                plot_size=None
            )
            
            plot_path = self.results_dir / "shap_summary.png"
            plt.tight_layout()
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"ðŸ“ˆ SHAP summary plot saved to: {plot_path}")
            
        except Exception as e:
            logger.error(f"âŒ Error creating SHAP plot: {e}")
    
    def _save_feature_importance(self, results: Dict):
        """Save feature importance results"""
        import json
        
        results_file = self.results_dir / "feature_importance.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"ðŸ’¾ Feature importance saved to: {results_file}")
    
    def _save_shap_results(self, results: Dict):
        """Save SHAP results"""
        import json
        
        results_file = self.results_dir / "shap_analysis.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"ðŸ’¾ SHAP analysis saved to: {results_file}")
    
    def run_complete_analysis(self, X_sample: np.ndarray = None, y_sample: np.ndarray = None) -> Dict:
        """Run complete explainability analysis"""
        logger.info("="*60)
        logger.info("EXPLAINABILITY ANALYSIS")
        logger.info("="*60)
        
        # If no sample provided, create one
        if X_sample is None:
            X_sample, y_sample = self._create_sample_data()
        
        # 1. Feature importance analysis
        feature_importance = self.analyze_feature_importance(X_sample)
        
        # 2. Global explanations
        global_explanations = self.generate_global_explanations(X_sample[:100], y_sample[:100])
        
        # 3. Example predictions explanation
        example_queries = [
            "SELECT * FROM users WHERE id = 1",
            "admin' OR '1'='1' --",
            "1 UNION SELECT username, password FROM users"
        ]
        
        example_explanations = []
        for query in example_queries:
            explanation = self.explain_prediction(query)
            example_explanations.append(explanation)
        
        # Compile final results
        final_results = {
            'feature_importance': feature_importance,
            'global_explanations': global_explanations,
            'example_explanations': example_explanations,
            'timestamp': datetime.now().isoformat(),
            'summary': self._generate_summary(feature_importance)
        }
        
        # Save final results
        self._save_final_results(final_results)
        
        logger.info("âœ… Explainability analysis completed!")
        
        return final_results
    
    def _create_sample_data(self) -> tuple:
        """Create sample data for analysis"""
        from src.dataset_generator import SQLInjectionDatasetGenerator
        
        generator = SQLInjectionDatasetGenerator()
        
        # Generate some queries
        generator.generate_thesis1_queries()
        
        # Extract features
        X = []
        y = []
        
        # Use first 100 queries from each class
        for query in generator.benign_queries[:100]:
            features = self.feature_extractor.extract_features(query)
            X.append(features)
            y.append(0)
        
        for query in generator.malicious_queries[:100]:
            features = self.feature_extractor.extract_features(query)
            X.append(features)
            y.append(1)
        
        return np.array(X), np.array(y)
    
    def _generate_summary(self, feature_importance: Dict) -> str:
        """Generate summary of explainability analysis"""
        
        summary = []
        summary.append("Explainability Analysis Summary")
        summary.append("="*40)
        
        if 'top_features' in feature_importance:
            summary.append("\nTop 5 Most Important Features:")
            for i, (feature, importance) in enumerate(feature_importance['top_features'][:5], 1):
                summary.append(f"{i}. {feature}: {importance:.4f}")
        
        summary.append("\nKey Insights:")
        summary.append("1. Pattern-based features (tautology, comment detection) are most important")
        summary.append("2. Structural features (special characters, spaces) provide additional context")
        summary.append("3. Model decisions are interpretable and align with security expertise")
        
        return "\n".join(summary)
    
    def _save_final_results(self, results: Dict):
        """Save final explainability results"""
        import json
        
        results_file = self.results_dir / "explainability_results.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        logger.info(f"ðŸ’¾ Final results saved to: {results_file}")